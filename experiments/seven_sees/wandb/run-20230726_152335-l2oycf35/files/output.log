/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch [1][5/1780], lr: 0.00000e+00, loss: 7.7103
Epoch [1][10/1780], lr: 0.00000e+00, loss: 7.97
Epoch [1][15/1780], lr: 0.00000e+00, loss: 7.5025
Epoch [1][20/1780], lr: 0.00000e+00, loss: 12.284
Epoch [1][25/1780], lr: 0.00000e+00, loss: 8.8654
Epoch [1][30/1780], lr: 0.00000e+00, loss: 5.924
Epoch [1][35/1780], lr: 0.00000e+00, loss: 7.8184
Epoch [1][40/1780], lr: 0.00000e+00, loss: 8.1632
Epoch [1][45/1780], lr: 0.00000e+00, loss: 12.04
Epoch [1][50/1780], lr: 0.00000e+00, loss: 6.2103
Epoch [1][55/1780], lr: 0.00000e+00, loss: 7.9126
Epoch [1][60/1780], lr: 0.00000e+00, loss: 8.6613
Epoch [1][65/1780], lr: 0.00000e+00, loss: 11.666
Epoch [1][70/1780], lr: 0.00000e+00, loss: 8.1021
Epoch [1][75/1780], lr: 0.00000e+00, loss: 15.473
Epoch [1][80/1780], lr: 0.00000e+00, loss: 10.229
Epoch [1][85/1780], lr: 0.00000e+00, loss: 7.2195
Epoch [1][90/1780], lr: 0.00000e+00, loss: 5.7338
Epoch [1][95/1780], lr: 0.00000e+00, loss: 5.7666
Epoch [1][100/1780], lr: 0.00000e+00, loss: 5.7744
Epoch [1][105/1780], lr: 0.00000e+00, loss: 8.3152
Epoch [1][110/1780], lr: 0.00000e+00, loss: 6.715
Epoch [1][115/1780], lr: 0.00000e+00, loss: 8.6563
Epoch [1][120/1780], lr: 0.00000e+00, loss: 9.1955
Epoch [1][125/1780], lr: 0.00000e+00, loss: 11.139
Epoch [1][130/1780], lr: 0.00000e+00, loss: 7.9317
Epoch [1][135/1780], lr: 0.00000e+00, loss: 7.553
Epoch [1][140/1780], lr: 0.00000e+00, loss: 5.7101
Epoch [1][145/1780], lr: 0.00000e+00, loss: 6.2839
Epoch [1][150/1780], lr: 0.00000e+00, loss: 10.187
Epoch [1][155/1780], lr: 0.00000e+00, loss: 9.645
Epoch [1][160/1780], lr: 0.00000e+00, loss: 8.0479
Epoch [1][165/1780], lr: 0.00000e+00, loss: 7.5228
Epoch [1][170/1780], lr: 0.00000e+00, loss: 7.8503
Epoch [1][175/1780], lr: 0.00000e+00, loss: 7.8909
Epoch [1][180/1780], lr: 0.00000e+00, loss: 7.3288
Epoch [1][185/1780], lr: 0.00000e+00, loss: 10.541
Epoch [1][190/1780], lr: 0.00000e+00, loss: 8.5594
Epoch [1][195/1780], lr: 0.00000e+00, loss: 12.606
Epoch [1][200/1780], lr: 0.00000e+00, loss: 5.5421
Epoch [1][205/1780], lr: 0.00000e+00, loss: 11.271
Epoch [1][210/1780], lr: 0.00000e+00, loss: 6.2056
Epoch [1][215/1780], lr: 0.00000e+00, loss: 8.5509
Epoch [1][220/1780], lr: 0.00000e+00, loss: 6.2573
Epoch [1][225/1780], lr: 0.00000e+00, loss: 7.3442
Epoch [1][230/1780], lr: 0.00000e+00, loss: 7.7524
Epoch [1][235/1780], lr: 0.00000e+00, loss: 7.1589
Epoch [1][240/1780], lr: 0.00000e+00, loss: 7.8924
Epoch [1][245/1780], lr: 0.00000e+00, loss: 6.5431
Epoch [1][250/1780], lr: 0.00000e+00, loss: 9.5761
Epoch [1][255/1780], lr: 0.00000e+00, loss: 5.1274
Epoch [1][260/1780], lr: 0.00000e+00, loss: 8.8649
Epoch [1][265/1780], lr: 0.00000e+00, loss: 6.3016
Epoch [1][270/1780], lr: 0.00000e+00, loss: 13.042
Epoch [1][275/1780], lr: 0.00000e+00, loss: 8.4335
Epoch [1][280/1780], lr: 0.00000e+00, loss: 9.5627
Epoch [1][285/1780], lr: 0.00000e+00, loss: 10.333
Epoch [1][290/1780], lr: 0.00000e+00, loss: 9.1276
Epoch [1][295/1780], lr: 0.00000e+00, loss: 6.143
Epoch [1][300/1780], lr: 0.00000e+00, loss: 10.078
Epoch [1][305/1780], lr: 0.00000e+00, loss: 8.0351
Epoch [1][310/1780], lr: 0.00000e+00, loss: 13.869
Epoch [1][315/1780], lr: 0.00000e+00, loss: 10.662
Epoch [1][320/1780], lr: 0.00000e+00, loss: 7.9868
Epoch [1][325/1780], lr: 0.00000e+00, loss: 8.8156
Epoch [1][330/1780], lr: 0.00000e+00, loss: 7.781
Epoch [1][335/1780], lr: 0.00000e+00, loss: 7.0717
Epoch [1][340/1780], lr: 0.00000e+00, loss: 9.1337
Epoch [1][345/1780], lr: 0.00000e+00, loss: 4.4681
Epoch [1][350/1780], lr: 0.00000e+00, loss: 11.153
Epoch [1][355/1780], lr: 0.00000e+00, loss: 8.2458
Epoch [1][360/1780], lr: 0.00000e+00, loss: 10.482
Epoch [1][365/1780], lr: 0.00000e+00, loss: 5.5502
Epoch [1][370/1780], lr: 0.00000e+00, loss: 6.0036
Epoch [1][375/1780], lr: 0.00000e+00, loss: 9.908
Epoch [1][380/1780], lr: 0.00000e+00, loss: 6.9922
Epoch [1][385/1780], lr: 0.00000e+00, loss: 6.5207
Epoch [1][390/1780], lr: 0.00000e+00, loss: 8.2753
Epoch [1][395/1780], lr: 0.00000e+00, loss: 10.888
Epoch [1][400/1780], lr: 0.00000e+00, loss: 8.715
Epoch [1][405/1780], lr: 0.00000e+00, loss: 7.9927
Epoch [1][410/1780], lr: 0.00000e+00, loss: 9.2797
Epoch [1][415/1780], lr: 0.00000e+00, loss: 7.6252
Epoch [1][420/1780], lr: 0.00000e+00, loss: 8.28
Epoch [1][425/1780], lr: 0.00000e+00, loss: 11.282
Epoch [1][430/1780], lr: 0.00000e+00, loss: 8.8484
Epoch [1][435/1780], lr: 0.00000e+00, loss: 7.3228
Epoch [1][440/1780], lr: 0.00000e+00, loss: 9.7657
Epoch [1][445/1780], lr: 0.00000e+00, loss: 9.0997
Epoch [1][450/1780], lr: 0.00000e+00, loss: 7.3486
Epoch [1][455/1780], lr: 0.00000e+00, loss: 7.9822
Epoch [1][460/1780], lr: 0.00000e+00, loss: 7.4063
Epoch [1][465/1780], lr: 0.00000e+00, loss: 6.7813
Epoch [1][470/1780], lr: 0.00000e+00, loss: 6.232
Epoch [1][475/1780], lr: 0.00000e+00, loss: 9.4202
Epoch [1][480/1780], lr: 0.00000e+00, loss: 8.08
Epoch [1][485/1780], lr: 0.00000e+00, loss: 10.172
Epoch [1][490/1780], lr: 0.00000e+00, loss: 8.1769
Epoch [1][495/1780], lr: 0.00000e+00, loss: 9.8199
Epoch [1][500/1780], lr: 0.00000e+00, loss: 9.6553
Epoch [1][505/1780], lr: 0.00000e+00, loss: 6.5203
Epoch [1][510/1780], lr: 0.00000e+00, loss: 10.094
Epoch [1][515/1780], lr: 0.00000e+00, loss: 8.9953
Epoch [1][520/1780], lr: 0.00000e+00, loss: 9.8355
Epoch [1][525/1780], lr: 0.00000e+00, loss: 6.1966
Epoch [1][530/1780], lr: 0.00000e+00, loss: 10.123
Epoch [1][535/1780], lr: 0.00000e+00, loss: 8.1245
Epoch [1][540/1780], lr: 0.00000e+00, loss: 8.379
Epoch [1][545/1780], lr: 0.00000e+00, loss: 8.8235
Epoch [1][550/1780], lr: 0.00000e+00, loss: 16.212
Epoch [1][555/1780], lr: 0.00000e+00, loss: 9.6251
Epoch [1][560/1780], lr: 0.00000e+00, loss: 11.219
Epoch [1][565/1780], lr: 0.00000e+00, loss: 9.6819
Epoch [1][570/1780], lr: 0.00000e+00, loss: 6.3942
Epoch [1][575/1780], lr: 0.00000e+00, loss: 9.6216
Epoch [1][580/1780], lr: 0.00000e+00, loss: 7.4063
Epoch [1][585/1780], lr: 0.00000e+00, loss: 9.312
Epoch [1][590/1780], lr: 0.00000e+00, loss: 9.1332
Epoch [1][595/1780], lr: 0.00000e+00, loss: 10.994
Epoch [1][600/1780], lr: 0.00000e+00, loss: 11.083
Epoch [1][605/1780], lr: 0.00000e+00, loss: 6.9763
Epoch [1][610/1780], lr: 0.00000e+00, loss: 7.5899
Epoch [1][615/1780], lr: 0.00000e+00, loss: 9.5012
Epoch [1][620/1780], lr: 0.00000e+00, loss: 9.365
Epoch [1][625/1780], lr: 0.00000e+00, loss: 7.6042
Epoch [1][630/1780], lr: 0.00000e+00, loss: 8.4771
Epoch [1][635/1780], lr: 0.00000e+00, loss: 9.2084
Epoch [1][640/1780], lr: 0.00000e+00, loss: 11.702
Epoch [1][645/1780], lr: 0.00000e+00, loss: 6.25
Epoch [1][650/1780], lr: 0.00000e+00, loss: 7.4071
Epoch [1][655/1780], lr: 0.00000e+00, loss: 10.584
Epoch [1][660/1780], lr: 0.00000e+00, loss: 13.188
Epoch [1][665/1780], lr: 0.00000e+00, loss: 11.48
Epoch [1][670/1780], lr: 0.00000e+00, loss: 8.6554
Epoch [1][675/1780], lr: 0.00000e+00, loss: 7.045
Epoch [1][680/1780], lr: 0.00000e+00, loss: 9.6544
Epoch [1][685/1780], lr: 0.00000e+00, loss: 9.0236
Epoch [1][690/1780], lr: 0.00000e+00, loss: 14.157
Epoch [1][695/1780], lr: 0.00000e+00, loss: 6.6052
Epoch [1][700/1780], lr: 0.00000e+00, loss: 8.2445
Epoch [1][705/1780], lr: 0.00000e+00, loss: 10.894
Epoch [1][710/1780], lr: 0.00000e+00, loss: 5.3653
Epoch [1][715/1780], lr: 0.00000e+00, loss: 14.401
Epoch [1][720/1780], lr: 0.00000e+00, loss: 6.3887
Epoch [1][725/1780], lr: 0.00000e+00, loss: 7.7076
Epoch [1][730/1780], lr: 0.00000e+00, loss: 9.4417
Epoch [1][735/1780], lr: 0.00000e+00, loss: 8.8677
Epoch [1][740/1780], lr: 0.00000e+00, loss: 5.6427
Epoch [1][745/1780], lr: 0.00000e+00, loss: 8.0586
Epoch [1][750/1780], lr: 0.00000e+00, loss: 8.0951
Epoch [1][755/1780], lr: 0.00000e+00, loss: 8.0744
Epoch [1][760/1780], lr: 0.00000e+00, loss: 11.509
Epoch [1][765/1780], lr: 0.00000e+00, loss: 8.7041
Epoch [1][770/1780], lr: 0.00000e+00, loss: 7.3142
Epoch [1][775/1780], lr: 0.00000e+00, loss: 10.362
Epoch [1][780/1780], lr: 0.00000e+00, loss: 9.8227
Epoch [1][785/1780], lr: 0.00000e+00, loss: 7.8336
Epoch [1][790/1780], lr: 0.00000e+00, loss: 11.501
Epoch [1][795/1780], lr: 0.00000e+00, loss: 6.5268
Epoch [1][800/1780], lr: 0.00000e+00, loss: 10.286
Epoch [1][805/1780], lr: 0.00000e+00, loss: 7.6722
Epoch [1][810/1780], lr: 0.00000e+00, loss: 10.386
Epoch [1][815/1780], lr: 0.00000e+00, loss: 13.504
Epoch [1][820/1780], lr: 0.00000e+00, loss: 8.7176
Epoch [1][825/1780], lr: 0.00000e+00, loss: 6.5625
Epoch [1][830/1780], lr: 0.00000e+00, loss: 9.5223
Epoch [1][835/1780], lr: 0.00000e+00, loss: 8.7008
Epoch [1][840/1780], lr: 0.00000e+00, loss: 12.622
Epoch [1][845/1780], lr: 0.00000e+00, loss: 6.29
Epoch [1][850/1780], lr: 0.00000e+00, loss: 10.608
Epoch [1][855/1780], lr: 0.00000e+00, loss: 8.9071
Epoch [1][860/1780], lr: 0.00000e+00, loss: 9.4113
Epoch [1][865/1780], lr: 0.00000e+00, loss: 11.033
Epoch [1][870/1780], lr: 0.00000e+00, loss: 11.844
Epoch [1][875/1780], lr: 0.00000e+00, loss: 11.817
Epoch [1][880/1780], lr: 0.00000e+00, loss: 7.1486
Epoch [1][885/1780], lr: 0.00000e+00, loss: 8.5686
Epoch [1][890/1780], lr: 0.00000e+00, loss: 10.794
Epoch [1][895/1780], lr: 0.00000e+00, loss: 8.3157
Epoch [1][900/1780], lr: 0.00000e+00, loss: 10.474
Epoch [1][905/1780], lr: 0.00000e+00, loss: 8.4188
Epoch [1][910/1780], lr: 0.00000e+00, loss: 8.9382
Epoch [1][915/1780], lr: 0.00000e+00, loss: 9.8574
Epoch [1][920/1780], lr: 0.00000e+00, loss: 10.738
Epoch [1][925/1780], lr: 0.00000e+00, loss: 9.6444
Epoch [1][930/1780], lr: 0.00000e+00, loss: 8.4456
Epoch [1][935/1780], lr: 0.00000e+00, loss: 11.616
Epoch [1][940/1780], lr: 0.00000e+00, loss: 9.0617
Epoch [1][945/1780], lr: 0.00000e+00, loss: 9.4044
Epoch [1][950/1780], lr: 0.00000e+00, loss: 7.0698
Epoch [1][955/1780], lr: 0.00000e+00, loss: 7.0715
Epoch [1][960/1780], lr: 0.00000e+00, loss: 10.052
Epoch [1][965/1780], lr: 0.00000e+00, loss: 5.8222
Epoch [1][970/1780], lr: 0.00000e+00, loss: 9.6328
Epoch [1][975/1780], lr: 0.00000e+00, loss: 11.841
Epoch [1][980/1780], lr: 0.00000e+00, loss: 8.3161
Epoch [1][985/1780], lr: 0.00000e+00, loss: 7.4799
Epoch [1][990/1780], lr: 0.00000e+00, loss: 8.7124
Epoch [1][995/1780], lr: 0.00000e+00, loss: 9.9009
Epoch [1][1000/1780], lr: 0.00000e+00, loss: 7.52
Epoch [1][1005/1780], lr: 0.00000e+00, loss: 8.1267
Epoch [1][1010/1780], lr: 0.00000e+00, loss: 12.068
Epoch [1][1015/1780], lr: 0.00000e+00, loss: 9.6406
Epoch [1][1020/1780], lr: 0.00000e+00, loss: 9.5116
Epoch [1][1025/1780], lr: 0.00000e+00, loss: 12.373
Epoch [1][1030/1780], lr: 0.00000e+00, loss: 7.6908
Epoch [1][1035/1780], lr: 0.00000e+00, loss: 12.377
Epoch [1][1040/1780], lr: 0.00000e+00, loss: 7.3862
Epoch [1][1045/1780], lr: 0.00000e+00, loss: 11.277
Epoch [1][1050/1780], lr: 0.00000e+00, loss: 12.84
Epoch [1][1055/1780], lr: 0.00000e+00, loss: 4.257
Epoch [1][1060/1780], lr: 0.00000e+00, loss: 5.8666
Epoch [1][1065/1780], lr: 0.00000e+00, loss: 6.2062
Epoch [1][1070/1780], lr: 0.00000e+00, loss: 8.0273
Epoch [1][1075/1780], lr: 0.00000e+00, loss: 10.867
Epoch [1][1080/1780], lr: 0.00000e+00, loss: 3.7751
Epoch [1][1085/1780], lr: 0.00000e+00, loss: 7.7696
Epoch [1][1090/1780], lr: 0.00000e+00, loss: 6.7957
Epoch [1][1095/1780], lr: 0.00000e+00, loss: 7.1612
Epoch [1][1100/1780], lr: 0.00000e+00, loss: 6.8224
Epoch [1][1105/1780], lr: 0.00000e+00, loss: 8.8533
Epoch [1][1110/1780], lr: 0.00000e+00, loss: 8.1276
Epoch [1][1115/1780], lr: 0.00000e+00, loss: 8.3768
Epoch [1][1120/1780], lr: 0.00000e+00, loss: 6.4334
Epoch [1][1125/1780], lr: 0.00000e+00, loss: 6.5009
Epoch [1][1130/1780], lr: 0.00000e+00, loss: 8.3868
Epoch [1][1135/1780], lr: 0.00000e+00, loss: 10.428
Epoch [1][1140/1780], lr: 0.00000e+00, loss: 8.3385
Epoch [1][1145/1780], lr: 0.00000e+00, loss: 6.657
Epoch [1][1150/1780], lr: 0.00000e+00, loss: 7.6134
Epoch [1][1155/1780], lr: 0.00000e+00, loss: 7.48
Epoch [1][1160/1780], lr: 0.00000e+00, loss: 9.0274
Epoch [1][1165/1780], lr: 0.00000e+00, loss: 9.2801
Epoch [1][1170/1780], lr: 0.00000e+00, loss: 8.8462
Epoch [1][1175/1780], lr: 0.00000e+00, loss: 5.5745
Epoch [1][1180/1780], lr: 0.00000e+00, loss: 7.9054
Epoch [1][1185/1780], lr: 0.00000e+00, loss: 15.188
Epoch [1][1190/1780], lr: 0.00000e+00, loss: 5.7117
Epoch [1][1195/1780], lr: 0.00000e+00, loss: 11.219
Epoch [1][1200/1780], lr: 0.00000e+00, loss: 5.553
Epoch [1][1205/1780], lr: 0.00000e+00, loss: 9.6643
Epoch [1][1210/1780], lr: 0.00000e+00, loss: 8.2085
Epoch [1][1215/1780], lr: 0.00000e+00, loss: 9.2752
Epoch [1][1220/1780], lr: 0.00000e+00, loss: 8.2649
Epoch [1][1225/1780], lr: 0.00000e+00, loss: 8.3643
Epoch [1][1230/1780], lr: 0.00000e+00, loss: 8.2344
Epoch [1][1235/1780], lr: 0.00000e+00, loss: 9.1974
Epoch [1][1240/1780], lr: 0.00000e+00, loss: 4.3004
Epoch [1][1245/1780], lr: 0.00000e+00, loss: 7.3645
Epoch [1][1250/1780], lr: 0.00000e+00, loss: 5.7863
Epoch [1][1255/1780], lr: 0.00000e+00, loss: 6.0067
Epoch [1][1260/1780], lr: 0.00000e+00, loss: 8.6
Epoch [1][1265/1780], lr: 0.00000e+00, loss: 7.386
Epoch [1][1270/1780], lr: 0.00000e+00, loss: 9.7579
Epoch [1][1275/1780], lr: 0.00000e+00, loss: 8.087
Epoch [1][1280/1780], lr: 0.00000e+00, loss: 7.8674
Epoch [1][1285/1780], lr: 0.00000e+00, loss: 13.247
Epoch [1][1290/1780], lr: 0.00000e+00, loss: 9.4978
Epoch [1][1295/1780], lr: 0.00000e+00, loss: 7.3456
Epoch [1][1300/1780], lr: 0.00000e+00, loss: 7.8786
Epoch [1][1305/1780], lr: 0.00000e+00, loss: 10.12
Epoch [1][1310/1780], lr: 0.00000e+00, loss: 8.1916
Epoch [1][1315/1780], lr: 0.00000e+00, loss: 8.0887
Epoch [1][1320/1780], lr: 0.00000e+00, loss: 9.3095
Epoch [1][1325/1780], lr: 0.00000e+00, loss: 6.7162
Epoch [1][1330/1780], lr: 0.00000e+00, loss: 10.928
Epoch [1][1335/1780], lr: 0.00000e+00, loss: 7.1257
Epoch [1][1340/1780], lr: 0.00000e+00, loss: 10.647
Epoch [1][1345/1780], lr: 0.00000e+00, loss: 7.9832
Epoch [1][1350/1780], lr: 0.00000e+00, loss: 7.9483
Epoch [1][1355/1780], lr: 0.00000e+00, loss: 6.8934
Epoch [1][1360/1780], lr: 0.00000e+00, loss: 8.9887
Epoch [1][1365/1780], lr: 0.00000e+00, loss: 7.2023
Epoch [1][1370/1780], lr: 0.00000e+00, loss: 10.614
Epoch [1][1375/1780], lr: 0.00000e+00, loss: 6.9458
Epoch [1][1380/1780], lr: 0.00000e+00, loss: 8.2703
Epoch [1][1385/1780], lr: 0.00000e+00, loss: 10.32
Epoch [1][1390/1780], lr: 0.00000e+00, loss: 6.4634
Epoch [1][1395/1780], lr: 0.00000e+00, loss: 13.967
Epoch [1][1400/1780], lr: 0.00000e+00, loss: 6.9861
Epoch [1][1405/1780], lr: 0.00000e+00, loss: 11.447
Epoch [1][1410/1780], lr: 0.00000e+00, loss: 8.6644
Epoch [1][1415/1780], lr: 0.00000e+00, loss: 11.151
Epoch [1][1420/1780], lr: 0.00000e+00, loss: 6.4842
Epoch [1][1425/1780], lr: 0.00000e+00, loss: 9.0589
Epoch [1][1430/1780], lr: 0.00000e+00, loss: 9.6698
Epoch [1][1435/1780], lr: 0.00000e+00, loss: 8.1998
Epoch [1][1440/1780], lr: 0.00000e+00, loss: 4.9178
Epoch [1][1445/1780], lr: 0.00000e+00, loss: 7.3408
Epoch [1][1450/1780], lr: 0.00000e+00, loss: 7.2454
Epoch [1][1455/1780], lr: 0.00000e+00, loss: 10.454
Epoch [1][1460/1780], lr: 0.00000e+00, loss: 6.6962
Epoch [1][1465/1780], lr: 0.00000e+00, loss: 6.2134
Epoch [1][1470/1780], lr: 0.00000e+00, loss: 9.7374
Epoch [1][1475/1780], lr: 0.00000e+00, loss: 7.751
Epoch [1][1480/1780], lr: 0.00000e+00, loss: 7.4262
Epoch [1][1485/1780], lr: 0.00000e+00, loss: 9.7301
Epoch [1][1490/1780], lr: 0.00000e+00, loss: 8.8141
Epoch [1][1495/1780], lr: 0.00000e+00, loss: 7.0499
Epoch [1][1500/1780], lr: 0.00000e+00, loss: 7.1107
Epoch [1][1505/1780], lr: 0.00000e+00, loss: 11.15
Epoch [1][1510/1780], lr: 0.00000e+00, loss: 7.3983
Epoch [1][1515/1780], lr: 0.00000e+00, loss: 7.054
Epoch [1][1520/1780], lr: 0.00000e+00, loss: 10.992
Epoch [1][1525/1780], lr: 0.00000e+00, loss: 10.531
Epoch [1][1530/1780], lr: 0.00000e+00, loss: 9.1914
Epoch [1][1535/1780], lr: 0.00000e+00, loss: 14.236
Epoch [1][1540/1780], lr: 0.00000e+00, loss: 13.277
Epoch [1][1545/1780], lr: 0.00000e+00, loss: 9.3113
Epoch [1][1550/1780], lr: 0.00000e+00, loss: 9.0488
Epoch [1][1555/1780], lr: 0.00000e+00, loss: 9.5066
Epoch [1][1560/1780], lr: 0.00000e+00, loss: 10.561
Epoch [1][1565/1780], lr: 0.00000e+00, loss: 8.352
Epoch [1][1570/1780], lr: 0.00000e+00, loss: 7.5517
Epoch [1][1575/1780], lr: 0.00000e+00, loss: 9.7965
Epoch [1][1580/1780], lr: 0.00000e+00, loss: 9.5524
Epoch [1][1585/1780], lr: 0.00000e+00, loss: 10.018
Epoch [1][1590/1780], lr: 0.00000e+00, loss: 10.283
Epoch [1][1595/1780], lr: 0.00000e+00, loss: 6.5336
Epoch [1][1600/1780], lr: 0.00000e+00, loss: 17.13
Epoch [1][1605/1780], lr: 0.00000e+00, loss: 5.8194
Epoch [1][1610/1780], lr: 0.00000e+00, loss: 10.585
Epoch [1][1615/1780], lr: 0.00000e+00, loss: 6.4134
Epoch [1][1620/1780], lr: 0.00000e+00, loss: 7.2547
Epoch [1][1625/1780], lr: 0.00000e+00, loss: 5.3501
Epoch [1][1630/1780], lr: 0.00000e+00, loss: 8.6838
Epoch [1][1635/1780], lr: 0.00000e+00, loss: 7.4075
Epoch [1][1640/1780], lr: 0.00000e+00, loss: 15.098
Epoch [1][1645/1780], lr: 0.00000e+00, loss: 6.3192
Epoch [1][1650/1780], lr: 0.00000e+00, loss: 9.6147
Epoch [1][1655/1780], lr: 0.00000e+00, loss: 6.3845
Epoch [1][1660/1780], lr: 0.00000e+00, loss: 8.248
Epoch [1][1665/1780], lr: 0.00000e+00, loss: 7.9008
Epoch [1][1670/1780], lr: 0.00000e+00, loss: 8.8178
Epoch [1][1675/1780], lr: 0.00000e+00, loss: 8.161
Epoch [1][1680/1780], lr: 0.00000e+00, loss: 12.956
Epoch [1][1685/1780], lr: 0.00000e+00, loss: 9.5162
Epoch [1][1690/1780], lr: 0.00000e+00, loss: 8.958
Epoch [1][1695/1780], lr: 0.00000e+00, loss: 9.5515
Epoch [1][1700/1780], lr: 0.00000e+00, loss: 11.909
Epoch [1][1705/1780], lr: 0.00000e+00, loss: 11.682
Epoch [1][1710/1780], lr: 0.00000e+00, loss: 10.665
Epoch [1][1715/1780], lr: 0.00000e+00, loss: 11.851
Epoch [1][1720/1780], lr: 0.00000e+00, loss: 11.773
Epoch [1][1725/1780], lr: 0.00000e+00, loss: 9.9975
Epoch [1][1730/1780], lr: 0.00000e+00, loss: 6.9103
Epoch [1][1735/1780], lr: 0.00000e+00, loss: 12.936
Epoch [1][1740/1780], lr: 0.00000e+00, loss: 5.7296
Epoch [1][1745/1780], lr: 0.00000e+00, loss: 5.7527
Epoch [1][1750/1780], lr: 0.00000e+00, loss: 3.6144
Epoch [1][1755/1780], lr: 0.00000e+00, loss: 10.829
Epoch [1][1760/1780], lr: 0.00000e+00, loss: 8.261
Epoch [1][1765/1780], lr: 0.00000e+00, loss: 6.55
Epoch [1][1770/1780], lr: 0.00000e+00, loss: 8.1964
Epoch [1][1775/1780], lr: 0.00000e+00, loss: 6.5699
Epoch [1][1780/1780], lr: 0.00000e+00, loss: 7.3557
Evaluating top_k_accuracy...
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
top1_acc: 0.02713, top5_acc: 0.1124, train_loss: 7.3557
Saving checkpoint at 1 epochs...
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch [2][5/1780], lr: 7.81250e-06, loss: 7.0285
Epoch [2][10/1780], lr: 7.81250e-06, loss: 7.3067
Epoch [2][15/1780], lr: 7.81250e-06, loss: 6.9219
Epoch [2][20/1780], lr: 7.81250e-06, loss: 8.1412
Epoch [2][25/1780], lr: 7.81250e-06, loss: 5.4192
Epoch [2][30/1780], lr: 7.81250e-06, loss: 6.3798
Epoch [2][35/1780], lr: 7.81250e-06, loss: 4.6899
Epoch [2][40/1780], lr: 7.81250e-06, loss: 5.6404
Epoch [2][45/1780], lr: 7.81250e-06, loss: 5.8908
Epoch [2][50/1780], lr: 7.81250e-06, loss: 5.8032
Epoch [2][55/1780], lr: 7.81250e-06, loss: 4.9985
Epoch [2][60/1780], lr: 7.81250e-06, loss: 5.0404
Epoch [2][65/1780], lr: 7.81250e-06, loss: 4.658
Epoch [2][70/1780], lr: 7.81250e-06, loss: 6.6507
Epoch [2][75/1780], lr: 7.81250e-06, loss: 5.3532
Epoch [2][80/1780], lr: 7.81250e-06, loss: 4.2752
Epoch [2][85/1780], lr: 7.81250e-06, loss: 5.0121
Epoch [2][90/1780], lr: 7.81250e-06, loss: 5.5462
Epoch [2][95/1780], lr: 7.81250e-06, loss: 4.3574
Epoch [2][100/1780], lr: 7.81250e-06, loss: 4.8806
Epoch [2][105/1780], lr: 7.81250e-06, loss: 5.283
Epoch [2][110/1780], lr: 7.81250e-06, loss: 3.5809
Epoch [2][115/1780], lr: 7.81250e-06, loss: 4.346
Epoch [2][120/1780], lr: 7.81250e-06, loss: 5.2406
Epoch [2][125/1780], lr: 7.81250e-06, loss: 4.3055
Epoch [2][130/1780], lr: 7.81250e-06, loss: 4.9088
Epoch [2][135/1780], lr: 7.81250e-06, loss: 3.4
Epoch [2][140/1780], lr: 7.81250e-06, loss: 5.3643
Epoch [2][145/1780], lr: 7.81250e-06, loss: 4.2139
Epoch [2][150/1780], lr: 7.81250e-06, loss: 4.4349
Epoch [2][155/1780], lr: 7.81250e-06, loss: 4.9949
Epoch [2][160/1780], lr: 7.81250e-06, loss: 3.7584
Epoch [2][165/1780], lr: 7.81250e-06, loss: 5.3116
Epoch [2][170/1780], lr: 7.81250e-06, loss: 4.6828
Epoch [2][175/1780], lr: 7.81250e-06, loss: 4.7899
Epoch [2][180/1780], lr: 7.81250e-06, loss: 4.5693
Epoch [2][185/1780], lr: 7.81250e-06, loss: 4.1032
Epoch [2][190/1780], lr: 7.81250e-06, loss: 3.7373
Epoch [2][195/1780], lr: 7.81250e-06, loss: 3.3808
Epoch [2][200/1780], lr: 7.81250e-06, loss: 4.6107
Epoch [2][205/1780], lr: 7.81250e-06, loss: 5.293
Epoch [2][210/1780], lr: 7.81250e-06, loss: 3.9025
Epoch [2][215/1780], lr: 7.81250e-06, loss: 4.724
Epoch [2][220/1780], lr: 7.81250e-06, loss: 5.4779
Epoch [2][225/1780], lr: 7.81250e-06, loss: 4.7023
Epoch [2][230/1780], lr: 7.81250e-06, loss: 4.0958
Epoch [2][235/1780], lr: 7.81250e-06, loss: 5.9698
Epoch [2][240/1780], lr: 7.81250e-06, loss: 3.9491
Epoch [2][245/1780], lr: 7.81250e-06, loss: 5.7056
Epoch [2][250/1780], lr: 7.81250e-06, loss: 5.1346
Epoch [2][255/1780], lr: 7.81250e-06, loss: 4.4588
Epoch [2][260/1780], lr: 7.81250e-06, loss: 3.9698
Epoch [2][265/1780], lr: 7.81250e-06, loss: 4.8946
Epoch [2][270/1780], lr: 7.81250e-06, loss: 4.6599
Epoch [2][275/1780], lr: 7.81250e-06, loss: 4.6377
Epoch [2][280/1780], lr: 7.81250e-06, loss: 3.8351
Epoch [2][285/1780], lr: 7.81250e-06, loss: 4.8858
Epoch [2][290/1780], lr: 7.81250e-06, loss: 3.9735
Epoch [2][295/1780], lr: 7.81250e-06, loss: 4.515
Epoch [2][300/1780], lr: 7.81250e-06, loss: 4.0542
Epoch [2][305/1780], lr: 7.81250e-06, loss: 4.2165
Epoch [2][310/1780], lr: 7.81250e-06, loss: 5.0555
Epoch [2][315/1780], lr: 7.81250e-06, loss: 4.099
Epoch [2][320/1780], lr: 7.81250e-06, loss: 4.291
Epoch [2][325/1780], lr: 7.81250e-06, loss: 5.4482
Epoch [2][330/1780], lr: 7.81250e-06, loss: 4.4037
Epoch [2][335/1780], lr: 7.81250e-06, loss: 4.9006
Epoch [2][340/1780], lr: 7.81250e-06, loss: 4.1505
Epoch [2][345/1780], lr: 7.81250e-06, loss: 4.1205
Epoch [2][350/1780], lr: 7.81250e-06, loss: 3.4375
Epoch [2][355/1780], lr: 7.81250e-06, loss: 4.1849
Epoch [2][360/1780], lr: 7.81250e-06, loss: 4.624
Epoch [2][365/1780], lr: 7.81250e-06, loss: 3.7613
Epoch [2][370/1780], lr: 7.81250e-06, loss: 4.7746
Epoch [2][375/1780], lr: 7.81250e-06, loss: 4.7454
Epoch [2][380/1780], lr: 7.81250e-06, loss: 5.0795
Epoch [2][385/1780], lr: 7.81250e-06, loss: 5.1189
Epoch [2][390/1780], lr: 7.81250e-06, loss: 4.0261
Epoch [2][395/1780], lr: 7.81250e-06, loss: 5.0713
Epoch [2][400/1780], lr: 7.81250e-06, loss: 4.4257
Epoch [2][405/1780], lr: 7.81250e-06, loss: 4.4612
Epoch [2][410/1780], lr: 7.81250e-06, loss: 5.1635
Epoch [2][415/1780], lr: 7.81250e-06, loss: 4.8072
Epoch [2][420/1780], lr: 7.81250e-06, loss: 3.0298
Epoch [2][425/1780], lr: 7.81250e-06, loss: 4.0124
Epoch [2][430/1780], lr: 7.81250e-06, loss: 4.5101
Epoch [2][435/1780], lr: 7.81250e-06, loss: 4.8895
Epoch [2][440/1780], lr: 7.81250e-06, loss: 4.5466
Epoch [2][445/1780], lr: 7.81250e-06, loss: 4.1754
Epoch [2][450/1780], lr: 7.81250e-06, loss: 4.0894
Epoch [2][455/1780], lr: 7.81250e-06, loss: 4.4908
Epoch [2][460/1780], lr: 7.81250e-06, loss: 6.6528
Epoch [2][465/1780], lr: 7.81250e-06, loss: 5.5559
Epoch [2][470/1780], lr: 7.81250e-06, loss: 3.3597
Epoch [2][475/1780], lr: 7.81250e-06, loss: 4.3477
Epoch [2][480/1780], lr: 7.81250e-06, loss: 4.6268
Epoch [2][485/1780], lr: 7.81250e-06, loss: 4.8099
Epoch [2][490/1780], lr: 7.81250e-06, loss: 4.2593
Epoch [2][495/1780], lr: 7.81250e-06, loss: 4.8637
Epoch [2][500/1780], lr: 7.81250e-06, loss: 4.8745
Epoch [2][505/1780], lr: 7.81250e-06, loss: 5.639
Epoch [2][510/1780], lr: 7.81250e-06, loss: 4.1361
Epoch [2][515/1780], lr: 7.81250e-06, loss: 3.5116
Epoch [2][520/1780], lr: 7.81250e-06, loss: 4.1562
Epoch [2][525/1780], lr: 7.81250e-06, loss: 3.6135
Epoch [2][530/1780], lr: 7.81250e-06, loss: 4.3583
Epoch [2][535/1780], lr: 7.81250e-06, loss: 4.4993
Epoch [2][540/1780], lr: 7.81250e-06, loss: 4.0043
Epoch [2][545/1780], lr: 7.81250e-06, loss: 3.8357
Epoch [2][550/1780], lr: 7.81250e-06, loss: 3.6293
Epoch [2][555/1780], lr: 7.81250e-06, loss: 4.1303
Epoch [2][560/1780], lr: 7.81250e-06, loss: 5.0406
Epoch [2][565/1780], lr: 7.81250e-06, loss: 2.8326
Epoch [2][570/1780], lr: 7.81250e-06, loss: 4.6208
Epoch [2][575/1780], lr: 7.81250e-06, loss: 3.7594
Epoch [2][580/1780], lr: 7.81250e-06, loss: 3.494
Epoch [2][585/1780], lr: 7.81250e-06, loss: 5.0441
Epoch [2][590/1780], lr: 7.81250e-06, loss: 4.4082
Epoch [2][595/1780], lr: 7.81250e-06, loss: 5.2842
Epoch [2][600/1780], lr: 7.81250e-06, loss: 3.9748
Epoch [2][605/1780], lr: 7.81250e-06, loss: 3.6165
Epoch [2][610/1780], lr: 7.81250e-06, loss: 4.5839
Epoch [2][615/1780], lr: 7.81250e-06, loss: 5.5633
Epoch [2][620/1780], lr: 7.81250e-06, loss: 4.5029
Epoch [2][625/1780], lr: 7.81250e-06, loss: 4.7825
Epoch [2][630/1780], lr: 7.81250e-06, loss: 4.3534
Epoch [2][635/1780], lr: 7.81250e-06, loss: 4.3284
Epoch [2][640/1780], lr: 7.81250e-06, loss: 3.6409
Epoch [2][645/1780], lr: 7.81250e-06, loss: 3.5621
Epoch [2][650/1780], lr: 7.81250e-06, loss: 4.8632
Epoch [2][655/1780], lr: 7.81250e-06, loss: 4.9844
Epoch [2][660/1780], lr: 7.81250e-06, loss: 5.5535
Epoch [2][665/1780], lr: 7.81250e-06, loss: 5.0334
Epoch [2][670/1780], lr: 7.81250e-06, loss: 4.8371
Epoch [2][675/1780], lr: 7.81250e-06, loss: 5.7152
Epoch [2][680/1780], lr: 7.81250e-06, loss: 4.5627
Epoch [2][685/1780], lr: 7.81250e-06, loss: 3.8524
Epoch [2][690/1780], lr: 7.81250e-06, loss: 3.6439
Epoch [2][695/1780], lr: 7.81250e-06, loss: 4.1881
Epoch [2][700/1780], lr: 7.81250e-06, loss: 3.6558
Epoch [2][705/1780], lr: 7.81250e-06, loss: 3.2809
Epoch [2][710/1780], lr: 7.81250e-06, loss: 3.6337
Epoch [2][715/1780], lr: 7.81250e-06, loss: 3.5922
Epoch [2][720/1780], lr: 7.81250e-06, loss: 4.6653
Epoch [2][725/1780], lr: 7.81250e-06, loss: 4.1874
Epoch [2][730/1780], lr: 7.81250e-06, loss: 4.0966
Epoch [2][735/1780], lr: 7.81250e-06, loss: 4.1014
Epoch [2][740/1780], lr: 7.81250e-06, loss: 3.7875
Epoch [2][745/1780], lr: 7.81250e-06, loss: 4.3266
Epoch [2][750/1780], lr: 7.81250e-06, loss: 4.2429
Epoch [2][755/1780], lr: 7.81250e-06, loss: 5.1942
Epoch [2][760/1780], lr: 7.81250e-06, loss: 5.8302
Epoch [2][765/1780], lr: 7.81250e-06, loss: 3.8747
Epoch [2][770/1780], lr: 7.81250e-06, loss: 3.6765
Epoch [2][775/1780], lr: 7.81250e-06, loss: 4.7386
Epoch [2][780/1780], lr: 7.81250e-06, loss: 4.7548
Epoch [2][785/1780], lr: 7.81250e-06, loss: 4.2925
Epoch [2][790/1780], lr: 7.81250e-06, loss: 4.3761
Epoch [2][795/1780], lr: 7.81250e-06, loss: 4.4873
Epoch [2][800/1780], lr: 7.81250e-06, loss: 3.7946
Epoch [2][805/1780], lr: 7.81250e-06, loss: 5.0522
Epoch [2][810/1780], lr: 7.81250e-06, loss: 4.2506
Epoch [2][815/1780], lr: 7.81250e-06, loss: 3.7953
Epoch [2][820/1780], lr: 7.81250e-06, loss: 3.0743
Epoch [2][825/1780], lr: 7.81250e-06, loss: 5.2683
Epoch [2][830/1780], lr: 7.81250e-06, loss: 4.6164
Epoch [2][835/1780], lr: 7.81250e-06, loss: 4.5671
Epoch [2][840/1780], lr: 7.81250e-06, loss: 4.4072
Epoch [2][845/1780], lr: 7.81250e-06, loss: 4.6147
Epoch [2][850/1780], lr: 7.81250e-06, loss: 3.4448
Epoch [2][855/1780], lr: 7.81250e-06, loss: 5.3955
Epoch [2][860/1780], lr: 7.81250e-06, loss: 3.9677
Epoch [2][865/1780], lr: 7.81250e-06, loss: 4.802
Epoch [2][870/1780], lr: 7.81250e-06, loss: 4.5785
Epoch [2][875/1780], lr: 7.81250e-06, loss: 4.4636
Epoch [2][880/1780], lr: 7.81250e-06, loss: 5.07
Epoch [2][885/1780], lr: 7.81250e-06, loss: 4.0666
Epoch [2][890/1780], lr: 7.81250e-06, loss: 4.101
Epoch [2][895/1780], lr: 7.81250e-06, loss: 2.9153
Epoch [2][900/1780], lr: 7.81250e-06, loss: 3.3511
Epoch [2][905/1780], lr: 7.81250e-06, loss: 4.2107
Epoch [2][910/1780], lr: 7.81250e-06, loss: 4.5965
Epoch [2][915/1780], lr: 7.81250e-06, loss: 3.2388
Epoch [2][920/1780], lr: 7.81250e-06, loss: 4.1859
Epoch [2][925/1780], lr: 7.81250e-06, loss: 3.4772
Epoch [2][930/1780], lr: 7.81250e-06, loss: 3.7196
Epoch [2][935/1780], lr: 7.81250e-06, loss: 3.7452
Epoch [2][940/1780], lr: 7.81250e-06, loss: 4.9222
Epoch [2][945/1780], lr: 7.81250e-06, loss: 4.8317
Epoch [2][950/1780], lr: 7.81250e-06, loss: 3.5366
Epoch [2][955/1780], lr: 7.81250e-06, loss: 4.3177
Epoch [2][960/1780], lr: 7.81250e-06, loss: 3.3388
Epoch [2][965/1780], lr: 7.81250e-06, loss: 3.5405
Epoch [2][970/1780], lr: 7.81250e-06, loss: 3.6526
Epoch [2][975/1780], lr: 7.81250e-06, loss: 4.9074
Epoch [2][980/1780], lr: 7.81250e-06, loss: 4.5424
Epoch [2][985/1780], lr: 7.81250e-06, loss: 5.0252
Epoch [2][990/1780], lr: 7.81250e-06, loss: 5.2163
Epoch [2][995/1780], lr: 7.81250e-06, loss: 3.2861
Epoch [2][1000/1780], lr: 7.81250e-06, loss: 3.9482
Epoch [2][1005/1780], lr: 7.81250e-06, loss: 4.6326
Epoch [2][1010/1780], lr: 7.81250e-06, loss: 3.7498
Epoch [2][1015/1780], lr: 7.81250e-06, loss: 3.6151
Epoch [2][1020/1780], lr: 7.81250e-06, loss: 3.5667
Epoch [2][1025/1780], lr: 7.81250e-06, loss: 4.1916
Epoch [2][1030/1780], lr: 7.81250e-06, loss: 3.8255
Epoch [2][1035/1780], lr: 7.81250e-06, loss: 4.5883
Epoch [2][1040/1780], lr: 7.81250e-06, loss: 2.6169
Epoch [2][1045/1780], lr: 7.81250e-06, loss: 4.5358
Epoch [2][1050/1780], lr: 7.81250e-06, loss: 4.866
Epoch [2][1055/1780], lr: 7.81250e-06, loss: 4.3505
Epoch [2][1060/1780], lr: 7.81250e-06, loss: 4.8821
Epoch [2][1065/1780], lr: 7.81250e-06, loss: 3.0964
Epoch [2][1070/1780], lr: 7.81250e-06, loss: 3.4227
Epoch [2][1075/1780], lr: 7.81250e-06, loss: 4.3898
Epoch [2][1080/1780], lr: 7.81250e-06, loss: 3.9513
Epoch [2][1085/1780], lr: 7.81250e-06, loss: 3.9671
Epoch [2][1090/1780], lr: 7.81250e-06, loss: 4.7752
Epoch [2][1095/1780], lr: 7.81250e-06, loss: 3.0629
Epoch [2][1100/1780], lr: 7.81250e-06, loss: 5.9747
Epoch [2][1105/1780], lr: 7.81250e-06, loss: 3.1137
Epoch [2][1110/1780], lr: 7.81250e-06, loss: 3.4417
Epoch [2][1115/1780], lr: 7.81250e-06, loss: 3.6276
Epoch [2][1120/1780], lr: 7.81250e-06, loss: 3.3714
Epoch [2][1125/1780], lr: 7.81250e-06, loss: 3.1877
Epoch [2][1130/1780], lr: 7.81250e-06, loss: 3.7487
Epoch [2][1135/1780], lr: 7.81250e-06, loss: 4.3026
Epoch [2][1140/1780], lr: 7.81250e-06, loss: 5.7892
Epoch [2][1145/1780], lr: 7.81250e-06, loss: 3.308
Epoch [2][1150/1780], lr: 7.81250e-06, loss: 4.3522
Epoch [2][1155/1780], lr: 7.81250e-06, loss: 2.8092
Epoch [2][1160/1780], lr: 7.81250e-06, loss: 3.6701
Epoch [2][1165/1780], lr: 7.81250e-06, loss: 5.0071
Epoch [2][1170/1780], lr: 7.81250e-06, loss: 4.7179
Epoch [2][1175/1780], lr: 7.81250e-06, loss: 3.5236
Epoch [2][1180/1780], lr: 7.81250e-06, loss: 3.7755
Epoch [2][1185/1780], lr: 7.81250e-06, loss: 4.232
Epoch [2][1190/1780], lr: 7.81250e-06, loss: 3.8094
Epoch [2][1195/1780], lr: 7.81250e-06, loss: 3.5923
Epoch [2][1200/1780], lr: 7.81250e-06, loss: 3.6874
Epoch [2][1205/1780], lr: 7.81250e-06, loss: 3.8119
Epoch [2][1210/1780], lr: 7.81250e-06, loss: 4.1717
Epoch [2][1215/1780], lr: 7.81250e-06, loss: 4.3755
Epoch [2][1220/1780], lr: 7.81250e-06, loss: 4.4365
Epoch [2][1225/1780], lr: 7.81250e-06, loss: 3.8331
Epoch [2][1230/1780], lr: 7.81250e-06, loss: 4.988
Epoch [2][1235/1780], lr: 7.81250e-06, loss: 3.9292
Epoch [2][1240/1780], lr: 7.81250e-06, loss: 3.924
Epoch [2][1245/1780], lr: 7.81250e-06, loss: 4.8282
Epoch [2][1250/1780], lr: 7.81250e-06, loss: 4.124
Epoch [2][1255/1780], lr: 7.81250e-06, loss: 4.137
Epoch [2][1260/1780], lr: 7.81250e-06, loss: 4.0439
Epoch [2][1265/1780], lr: 7.81250e-06, loss: 5.3135
Epoch [2][1270/1780], lr: 7.81250e-06, loss: 3.6991
Epoch [2][1275/1780], lr: 7.81250e-06, loss: 3.583
Epoch [2][1280/1780], lr: 7.81250e-06, loss: 4.0488
Epoch [2][1285/1780], lr: 7.81250e-06, loss: 4.5131
Epoch [2][1290/1780], lr: 7.81250e-06, loss: 3.6299
Epoch [2][1295/1780], lr: 7.81250e-06, loss: 3.9077
Epoch [2][1300/1780], lr: 7.81250e-06, loss: 4.6594
Epoch [2][1305/1780], lr: 7.81250e-06, loss: 4.9093
Epoch [2][1310/1780], lr: 7.81250e-06, loss: 3.2043
Epoch [2][1315/1780], lr: 7.81250e-06, loss: 4.3799
Epoch [2][1320/1780], lr: 7.81250e-06, loss: 4.5691
Epoch [2][1325/1780], lr: 7.81250e-06, loss: 3.9612
Epoch [2][1330/1780], lr: 7.81250e-06, loss: 3.1758
Epoch [2][1335/1780], lr: 7.81250e-06, loss: 4.2524
Epoch [2][1340/1780], lr: 7.81250e-06, loss: 4.7197
Epoch [2][1345/1780], lr: 7.81250e-06, loss: 4.2492
Epoch [2][1350/1780], lr: 7.81250e-06, loss: 5.6237
Epoch [2][1355/1780], lr: 7.81250e-06, loss: 3.4657
Epoch [2][1360/1780], lr: 7.81250e-06, loss: 3.7075
Epoch [2][1365/1780], lr: 7.81250e-06, loss: 4.6903
Epoch [2][1370/1780], lr: 7.81250e-06, loss: 3.8997
Epoch [2][1375/1780], lr: 7.81250e-06, loss: 4.0609
Epoch [2][1380/1780], lr: 7.81250e-06, loss: 3.5368
Epoch [2][1385/1780], lr: 7.81250e-06, loss: 3.9712
Epoch [2][1390/1780], lr: 7.81250e-06, loss: 4.1828
Epoch [2][1395/1780], lr: 7.81250e-06, loss: 4.2046
Epoch [2][1400/1780], lr: 7.81250e-06, loss: 4.1399
Epoch [2][1405/1780], lr: 7.81250e-06, loss: 4.8325
Epoch [2][1410/1780], lr: 7.81250e-06, loss: 4.3431
Epoch [2][1415/1780], lr: 7.81250e-06, loss: 3.9008
Epoch [2][1420/1780], lr: 7.81250e-06, loss: 4.0216
Epoch [2][1425/1780], lr: 7.81250e-06, loss: 3.6278
Epoch [2][1430/1780], lr: 7.81250e-06, loss: 3.9223
Epoch [2][1435/1780], lr: 7.81250e-06, loss: 5.3362
Epoch [2][1440/1780], lr: 7.81250e-06, loss: 3.962
Epoch [2][1445/1780], lr: 7.81250e-06, loss: 4.4454
Epoch [2][1450/1780], lr: 7.81250e-06, loss: 3.6121
Epoch [2][1455/1780], lr: 7.81250e-06, loss: 4.3317
Epoch [2][1460/1780], lr: 7.81250e-06, loss: 3.904
Epoch [2][1465/1780], lr: 7.81250e-06, loss: 3.6792
Epoch [2][1470/1780], lr: 7.81250e-06, loss: 4.1135
Epoch [2][1475/1780], lr: 7.81250e-06, loss: 4.0667
Epoch [2][1480/1780], lr: 7.81250e-06, loss: 4.0998
Epoch [2][1485/1780], lr: 7.81250e-06, loss: 3.5819
Epoch [2][1490/1780], lr: 7.81250e-06, loss: 2.7805
Epoch [2][1495/1780], lr: 7.81250e-06, loss: 3.5227
Epoch [2][1500/1780], lr: 7.81250e-06, loss: 3.2378
Epoch [2][1505/1780], lr: 7.81250e-06, loss: 4.1827
Epoch [2][1510/1780], lr: 7.81250e-06, loss: 4.4019
Epoch [2][1515/1780], lr: 7.81250e-06, loss: 4.5619
Epoch [2][1520/1780], lr: 7.81250e-06, loss: 4.4834
Epoch [2][1525/1780], lr: 7.81250e-06, loss: 3.8396
Epoch [2][1530/1780], lr: 7.81250e-06, loss: 3.8985
Epoch [2][1535/1780], lr: 7.81250e-06, loss: 3.4227
Epoch [2][1540/1780], lr: 7.81250e-06, loss: 4.3137
Epoch [2][1545/1780], lr: 7.81250e-06, loss: 4.0056
Epoch [2][1550/1780], lr: 7.81250e-06, loss: 3.8245
Epoch [2][1555/1780], lr: 7.81250e-06, loss: 4.0137
Epoch [2][1560/1780], lr: 7.81250e-06, loss: 4.9584
Epoch [2][1565/1780], lr: 7.81250e-06, loss: 4.1793
Epoch [2][1570/1780], lr: 7.81250e-06, loss: 5.6504
Epoch [2][1575/1780], lr: 7.81250e-06, loss: 3.9009
Epoch [2][1580/1780], lr: 7.81250e-06, loss: 2.8956
Epoch [2][1585/1780], lr: 7.81250e-06, loss: 3.9371
Epoch [2][1590/1780], lr: 7.81250e-06, loss: 3.1876
Epoch [2][1595/1780], lr: 7.81250e-06, loss: 4.3309
Epoch [2][1600/1780], lr: 7.81250e-06, loss: 4.7479
Epoch [2][1605/1780], lr: 7.81250e-06, loss: 4.7492
Epoch [2][1610/1780], lr: 7.81250e-06, loss: 3.5852
Epoch [2][1615/1780], lr: 7.81250e-06, loss: 4.2044
Epoch [2][1620/1780], lr: 7.81250e-06, loss: 3.89
Epoch [2][1625/1780], lr: 7.81250e-06, loss: 2.6637
Epoch [2][1630/1780], lr: 7.81250e-06, loss: 4.0841
Epoch [2][1635/1780], lr: 7.81250e-06, loss: 3.4741
Epoch [2][1640/1780], lr: 7.81250e-06, loss: 4.033
Epoch [2][1645/1780], lr: 7.81250e-06, loss: 4.3176
Epoch [2][1650/1780], lr: 7.81250e-06, loss: 5.5909
Epoch [2][1655/1780], lr: 7.81250e-06, loss: 4.2698
Epoch [2][1660/1780], lr: 7.81250e-06, loss: 3.8032
Epoch [2][1665/1780], lr: 7.81250e-06, loss: 3.892
Epoch [2][1670/1780], lr: 7.81250e-06, loss: 5.0054
Epoch [2][1675/1780], lr: 7.81250e-06, loss: 3.0983
Epoch [2][1680/1780], lr: 7.81250e-06, loss: 3.0705
Epoch [2][1685/1780], lr: 7.81250e-06, loss: 5.78
Epoch [2][1690/1780], lr: 7.81250e-06, loss: 3.8925
Epoch [2][1695/1780], lr: 7.81250e-06, loss: 4.239
Epoch [2][1700/1780], lr: 7.81250e-06, loss: 3.8171
Epoch [2][1705/1780], lr: 7.81250e-06, loss: 3.9438
Epoch [2][1710/1780], lr: 7.81250e-06, loss: 4.3402
Epoch [2][1715/1780], lr: 7.81250e-06, loss: 3.7212
Epoch [2][1720/1780], lr: 7.81250e-06, loss: 2.9856
Epoch [2][1725/1780], lr: 7.81250e-06, loss: 2.9535
Epoch [2][1730/1780], lr: 7.81250e-06, loss: 5.1733
Epoch [2][1735/1780], lr: 7.81250e-06, loss: 3.8771
Epoch [2][1740/1780], lr: 7.81250e-06, loss: 3.2244
Epoch [2][1745/1780], lr: 7.81250e-06, loss: 3.9585
Epoch [2][1750/1780], lr: 7.81250e-06, loss: 3.9632
Epoch [2][1755/1780], lr: 7.81250e-06, loss: 4.092
Epoch [2][1760/1780], lr: 7.81250e-06, loss: 4.1372
Epoch [2][1765/1780], lr: 7.81250e-06, loss: 3.7209
Epoch [2][1770/1780], lr: 7.81250e-06, loss: 4.6936
Epoch [2][1775/1780], lr: 7.81250e-06, loss: 5.0123
Epoch [2][1780/1780], lr: 7.81250e-06, loss: 5.0581
Evaluating top_k_accuracy...
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
top1_acc: 0.1279, top5_acc: 0.3605, train_loss: 5.0581
Saving checkpoint at 2 epochs...
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/home/myuser1/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch [3][5/1780], lr: 1.56250e-05, loss: 4.241
Epoch [3][10/1780], lr: 1.56250e-05, loss: 4.1719
Epoch [3][15/1780], lr: 1.56250e-05, loss: 4.1483
Epoch [3][20/1780], lr: 1.56250e-05, loss: 2.8023
Epoch [3][25/1780], lr: 1.56250e-05, loss: 4.636
Epoch [3][30/1780], lr: 1.56250e-05, loss: 3.3082
Epoch [3][35/1780], lr: 1.56250e-05, loss: 3.6253
Epoch [3][40/1780], lr: 1.56250e-05, loss: 4.2211
Epoch [3][45/1780], lr: 1.56250e-05, loss: 2.6797
Epoch [3][50/1780], lr: 1.56250e-05, loss: 2.9811
Epoch [3][55/1780], lr: 1.56250e-05, loss: 3.2206
Epoch [3][60/1780], lr: 1.56250e-05, loss: 3.5778
Epoch [3][65/1780], lr: 1.56250e-05, loss: 4.2627
Epoch [3][70/1780], lr: 1.56250e-05, loss: 3.8667
Epoch [3][75/1780], lr: 1.56250e-05, loss: 3.5565
Epoch [3][80/1780], lr: 1.56250e-05, loss: 3.6308
Epoch [3][85/1780], lr: 1.56250e-05, loss: 4.1588
Epoch [3][90/1780], lr: 1.56250e-05, loss: 4.0081
Epoch [3][95/1780], lr: 1.56250e-05, loss: 4.5462
Epoch [3][100/1780], lr: 1.56250e-05, loss: 4.5965
Epoch [3][105/1780], lr: 1.56250e-05, loss: 3.3222
Epoch [3][110/1780], lr: 1.56250e-05, loss: 4.361
Epoch [3][115/1780], lr: 1.56250e-05, loss: 3.9868
Epoch [3][120/1780], lr: 1.56250e-05, loss: 3.5471
Epoch [3][125/1780], lr: 1.56250e-05, loss: 3.2859
Epoch [3][130/1780], lr: 1.56250e-05, loss: 3.9411
Epoch [3][135/1780], lr: 1.56250e-05, loss: 3.8188
Epoch [3][140/1780], lr: 1.56250e-05, loss: 2.9497
Epoch [3][145/1780], lr: 1.56250e-05, loss: 4.503
Epoch [3][150/1780], lr: 1.56250e-05, loss: 4.9075
Epoch [3][155/1780], lr: 1.56250e-05, loss: 3.2539
Epoch [3][160/1780], lr: 1.56250e-05, loss: 2.9884
Epoch [3][165/1780], lr: 1.56250e-05, loss: 3.8517
Epoch [3][170/1780], lr: 1.56250e-05, loss: 2.8846
Epoch [3][175/1780], lr: 1.56250e-05, loss: 2.9297
Epoch [3][180/1780], lr: 1.56250e-05, loss: 3.5574
Epoch [3][185/1780], lr: 1.56250e-05, loss: 3.0051
Epoch [3][190/1780], lr: 1.56250e-05, loss: 3.6304
Epoch [3][195/1780], lr: 1.56250e-05, loss: 3.7954
Epoch [3][200/1780], lr: 1.56250e-05, loss: 3.0325
Epoch [3][205/1780], lr: 1.56250e-05, loss: 3.2166
Epoch [3][210/1780], lr: 1.56250e-05, loss: 3.4452
Epoch [3][215/1780], lr: 1.56250e-05, loss: 3.5843
Epoch [3][220/1780], lr: 1.56250e-05, loss: 3.3322
Epoch [3][225/1780], lr: 1.56250e-05, loss: 3.3149
Epoch [3][230/1780], lr: 1.56250e-05, loss: 4.8907
Epoch [3][235/1780], lr: 1.56250e-05, loss: 2.7494
Epoch [3][240/1780], lr: 1.56250e-05, loss: 3.5271
Epoch [3][245/1780], lr: 1.56250e-05, loss: 3.4687
Epoch [3][250/1780], lr: 1.56250e-05, loss: 3.5554
Epoch [3][255/1780], lr: 1.56250e-05, loss: 1.8955
Epoch [3][260/1780], lr: 1.56250e-05, loss: 2.6908
Epoch [3][265/1780], lr: 1.56250e-05, loss: 4.254
Epoch [3][270/1780], lr: 1.56250e-05, loss: 3.4638
Epoch [3][275/1780], lr: 1.56250e-05, loss: 2.8982
Epoch [3][280/1780], lr: 1.56250e-05, loss: 3.4756
Epoch [3][285/1780], lr: 1.56250e-05, loss: 4.1207
Epoch [3][290/1780], lr: 1.56250e-05, loss: 2.898
Epoch [3][295/1780], lr: 1.56250e-05, loss: 2.458
Epoch [3][300/1780], lr: 1.56250e-05, loss: 3.0796
Epoch [3][305/1780], lr: 1.56250e-05, loss: 2.9052
Epoch [3][310/1780], lr: 1.56250e-05, loss: 4.9268
Epoch [3][315/1780], lr: 1.56250e-05, loss: 3.5326
Epoch [3][320/1780], lr: 1.56250e-05, loss: 5.409
Epoch [3][325/1780], lr: 1.56250e-05, loss: 4.2933
Epoch [3][330/1780], lr: 1.56250e-05, loss: 4.1676
Epoch [3][335/1780], lr: 1.56250e-05, loss: 4.4451
Epoch [3][340/1780], lr: 1.56250e-05, loss: 3.3811
Epoch [3][345/1780], lr: 1.56250e-05, loss: 4.2354
Epoch [3][350/1780], lr: 1.56250e-05, loss: 4.7336
Epoch [3][355/1780], lr: 1.56250e-05, loss: 3.8127
Epoch [3][360/1780], lr: 1.56250e-05, loss: 3.3673
Epoch [3][365/1780], lr: 1.56250e-05, loss: 5.059
Epoch [3][370/1780], lr: 1.56250e-05, loss: 3.8393
Epoch [3][375/1780], lr: 1.56250e-05, loss: 4.288
Epoch [3][380/1780], lr: 1.56250e-05, loss: 3.8013
Epoch [3][385/1780], lr: 1.56250e-05, loss: 3.8853
Epoch [3][390/1780], lr: 1.56250e-05, loss: 5.1576
Epoch [3][395/1780], lr: 1.56250e-05, loss: 3.5831
Epoch [3][400/1780], lr: 1.56250e-05, loss: 2.3673
Epoch [3][405/1780], lr: 1.56250e-05, loss: 4.6096
Epoch [3][410/1780], lr: 1.56250e-05, loss: 3.4464
Epoch [3][415/1780], lr: 1.56250e-05, loss: 4.3692
Epoch [3][420/1780], lr: 1.56250e-05, loss: 4.6031
Epoch [3][425/1780], lr: 1.56250e-05, loss: 4.1363
Epoch [3][430/1780], lr: 1.56250e-05, loss: 4.117
Epoch [3][435/1780], lr: 1.56250e-05, loss: 4.7458
Epoch [3][440/1780], lr: 1.56250e-05, loss: 4.2964
Epoch [3][445/1780], lr: 1.56250e-05, loss: 4.3171
Epoch [3][450/1780], lr: 1.56250e-05, loss: 4.5967
Epoch [3][455/1780], lr: 1.56250e-05, loss: 3.328
Epoch [3][460/1780], lr: 1.56250e-05, loss: 4.2867
Epoch [3][465/1780], lr: 1.56250e-05, loss: 3.5378
Epoch [3][470/1780], lr: 1.56250e-05, loss: 3.9806
Epoch [3][475/1780], lr: 1.56250e-05, loss: 6.1824
Epoch [3][480/1780], lr: 1.56250e-05, loss: 3.6349
Epoch [3][485/1780], lr: 1.56250e-05, loss: 3.6969
Epoch [3][490/1780], lr: 1.56250e-05, loss: 4.3335
Epoch [3][495/1780], lr: 1.56250e-05, loss: 3.7672
Epoch [3][500/1780], lr: 1.56250e-05, loss: 3.641
Epoch [3][505/1780], lr: 1.56250e-05, loss: 3.9012
Epoch [3][510/1780], lr: 1.56250e-05, loss: 4.8118
Epoch [3][515/1780], lr: 1.56250e-05, loss: 4.1818
Epoch [3][520/1780], lr: 1.56250e-05, loss: 3.4371
Epoch [3][525/1780], lr: 1.56250e-05, loss: 4.0821
Epoch [3][530/1780], lr: 1.56250e-05, loss: 3.4196
Epoch [3][535/1780], lr: 1.56250e-05, loss: 3.7991
Epoch [3][540/1780], lr: 1.56250e-05, loss: 4.2859
Epoch [3][545/1780], lr: 1.56250e-05, loss: 3.3637
Epoch [3][550/1780], lr: 1.56250e-05, loss: 4.2002
Epoch [3][555/1780], lr: 1.56250e-05, loss: 3.4838
Epoch [3][560/1780], lr: 1.56250e-05, loss: 3.0911
Epoch [3][565/1780], lr: 1.56250e-05, loss: 4.4444
Epoch [3][570/1780], lr: 1.56250e-05, loss: 3.9303
Epoch [3][575/1780], lr: 1.56250e-05, loss: 3.6111
Epoch [3][580/1780], lr: 1.56250e-05, loss: 4.1048
Epoch [3][585/1780], lr: 1.56250e-05, loss: 4.3231
Epoch [3][590/1780], lr: 1.56250e-05, loss: 3.8489
Epoch [3][595/1780], lr: 1.56250e-05, loss: 4.5925
Epoch [3][600/1780], lr: 1.56250e-05, loss: 3.8968
Epoch [3][605/1780], lr: 1.56250e-05, loss: 3.8209
Epoch [3][610/1780], lr: 1.56250e-05, loss: 3.0186
Epoch [3][615/1780], lr: 1.56250e-05, loss: 3.1345
Epoch [3][620/1780], lr: 1.56250e-05, loss: 3.4489
Epoch [3][625/1780], lr: 1.56250e-05, loss: 4.8267
Epoch [3][630/1780], lr: 1.56250e-05, loss: 2.9834
Epoch [3][635/1780], lr: 1.56250e-05, loss: 3.6827
Epoch [3][640/1780], lr: 1.56250e-05, loss: 2.6026
Epoch [3][645/1780], lr: 1.56250e-05, loss: 3.6248
Epoch [3][650/1780], lr: 1.56250e-05, loss: 3.3955
Epoch [3][655/1780], lr: 1.56250e-05, loss: 3.5867
Epoch [3][660/1780], lr: 1.56250e-05, loss: 4.2537
Epoch [3][665/1780], lr: 1.56250e-05, loss: 5.4627
Epoch [3][670/1780], lr: 1.56250e-05, loss: 4.1697
Epoch [3][675/1780], lr: 1.56250e-05, loss: 2.7957
Epoch [3][680/1780], lr: 1.56250e-05, loss: 3.1953
Epoch [3][685/1780], lr: 1.56250e-05, loss: 2.857
Epoch [3][690/1780], lr: 1.56250e-05, loss: 3.1613
Epoch [3][695/1780], lr: 1.56250e-05, loss: 3.8395
Epoch [3][700/1780], lr: 1.56250e-05, loss: 3.4919
Epoch [3][705/1780], lr: 1.56250e-05, loss: 4.8399
Epoch [3][710/1780], lr: 1.56250e-05, loss: 4.2376
Epoch [3][715/1780], lr: 1.56250e-05, loss: 2.217
Epoch [3][720/1780], lr: 1.56250e-05, loss: 4.1351
Epoch [3][725/1780], lr: 1.56250e-05, loss: 3.5838
Epoch [3][730/1780], lr: 1.56250e-05, loss: 3.2287
Epoch [3][735/1780], lr: 1.56250e-05, loss: 3.6957
Epoch [3][740/1780], lr: 1.56250e-05, loss: 3.241
Epoch [3][745/1780], lr: 1.56250e-05, loss: 3.8913
Epoch [3][750/1780], lr: 1.56250e-05, loss: 2.9978
Epoch [3][755/1780], lr: 1.56250e-05, loss: 3.5508
Epoch [3][760/1780], lr: 1.56250e-05, loss: 3.5246
Epoch [3][765/1780], lr: 1.56250e-05, loss: 5.3264
Epoch [3][770/1780], lr: 1.56250e-05, loss: 4.813
Epoch [3][775/1780], lr: 1.56250e-05, loss: 3.7709
Epoch [3][780/1780], lr: 1.56250e-05, loss: 4.6244
Epoch [3][785/1780], lr: 1.56250e-05, loss: 5.385
Epoch [3][790/1780], lr: 1.56250e-05, loss: 2.9813
Epoch [3][795/1780], lr: 1.56250e-05, loss: 3.2916
Epoch [3][800/1780], lr: 1.56250e-05, loss: 4.1928
Epoch [3][805/1780], lr: 1.56250e-05, loss: 2.1567
Epoch [3][810/1780], lr: 1.56250e-05, loss: 2.3209
Epoch [3][815/1780], lr: 1.56250e-05, loss: 2.9136
Epoch [3][820/1780], lr: 1.56250e-05, loss: 4.1963
Epoch [3][825/1780], lr: 1.56250e-05, loss: 2.0722
Epoch [3][830/1780], lr: 1.56250e-05, loss: 3.5473
Epoch [3][835/1780], lr: 1.56250e-05, loss: 3.2222
Epoch [3][840/1780], lr: 1.56250e-05, loss: 3.5157
Epoch [3][845/1780], lr: 1.56250e-05, loss: 3.666
Epoch [3][850/1780], lr: 1.56250e-05, loss: 4.0737
Epoch [3][855/1780], lr: 1.56250e-05, loss: 1.9138
Epoch [3][860/1780], lr: 1.56250e-05, loss: 4.6742
Epoch [3][865/1780], lr: 1.56250e-05, loss: 3.2631
Epoch [3][870/1780], lr: 1.56250e-05, loss: 3.0895
Epoch [3][875/1780], lr: 1.56250e-05, loss: 4.9747
Epoch [3][880/1780], lr: 1.56250e-05, loss: 4.5834
Epoch [3][885/1780], lr: 1.56250e-05, loss: 4.1578
Epoch [3][890/1780], lr: 1.56250e-05, loss: 3.585
Epoch [3][895/1780], lr: 1.56250e-05, loss: 3.063
Epoch [3][900/1780], lr: 1.56250e-05, loss: 3.5852
Epoch [3][905/1780], lr: 1.56250e-05, loss: 3.1902
Epoch [3][910/1780], lr: 1.56250e-05, loss: 5.0104
Epoch [3][915/1780], lr: 1.56250e-05, loss: 4.1479
Epoch [3][920/1780], lr: 1.56250e-05, loss: 4.9917
Epoch [3][925/1780], lr: 1.56250e-05, loss: 4.7728
Epoch [3][930/1780], lr: 1.56250e-05, loss: 4.1175
Epoch [3][935/1780], lr: 1.56250e-05, loss: 3.2572
Epoch [3][940/1780], lr: 1.56250e-05, loss: 3.9848
Epoch [3][945/1780], lr: 1.56250e-05, loss: 3.9102
Epoch [3][950/1780], lr: 1.56250e-05, loss: 3.524
Epoch [3][955/1780], lr: 1.56250e-05, loss: 2.9876
Epoch [3][960/1780], lr: 1.56250e-05, loss: 4.7791
Epoch [3][965/1780], lr: 1.56250e-05, loss: 3.4366
Epoch [3][970/1780], lr: 1.56250e-05, loss: 3.6008
Epoch [3][975/1780], lr: 1.56250e-05, loss: 4.3416
Epoch [3][980/1780], lr: 1.56250e-05, loss: 4.3501
Epoch [3][985/1780], lr: 1.56250e-05, loss: 3.9622
Epoch [3][990/1780], lr: 1.56250e-05, loss: 2.5
Epoch [3][995/1780], lr: 1.56250e-05, loss: 3.9874
Epoch [3][1000/1780], lr: 1.56250e-05, loss: 3.417
Epoch [3][1005/1780], lr: 1.56250e-05, loss: 2.491
Epoch [3][1010/1780], lr: 1.56250e-05, loss: 4.7525
Epoch [3][1015/1780], lr: 1.56250e-05, loss: 3.2757
Epoch [3][1020/1780], lr: 1.56250e-05, loss: 2.927
Epoch [3][1025/1780], lr: 1.56250e-05, loss: 3.2605
Epoch [3][1030/1780], lr: 1.56250e-05, loss: 3.9437
Epoch [3][1035/1780], lr: 1.56250e-05, loss: 2.7448
Epoch [3][1040/1780], lr: 1.56250e-05, loss: 3.289
Epoch [3][1045/1780], lr: 1.56250e-05, loss: 4.9108
Epoch [3][1050/1780], lr: 1.56250e-05, loss: 2.1222
Epoch [3][1055/1780], lr: 1.56250e-05, loss: 3.1184
Epoch [3][1060/1780], lr: 1.56250e-05, loss: 3.5373
Epoch [3][1065/1780], lr: 1.56250e-05, loss: 4.1438
Epoch [3][1070/1780], lr: 1.56250e-05, loss: 2.3535
Epoch [3][1075/1780], lr: 1.56250e-05, loss: 4.1105
Epoch [3][1080/1780], lr: 1.56250e-05, loss: 5.0002
Epoch [3][1085/1780], lr: 1.56250e-05, loss: 3.6782
Epoch [3][1090/1780], lr: 1.56250e-05, loss: 3.7269
Epoch [3][1095/1780], lr: 1.56250e-05, loss: 3.5879
Epoch [3][1100/1780], lr: 1.56250e-05, loss: 4.7276
Epoch [3][1105/1780], lr: 1.56250e-05, loss: 2.418
Epoch [3][1110/1780], lr: 1.56250e-05, loss: 3.2731
Epoch [3][1115/1780], lr: 1.56250e-05, loss: 3.7642
Epoch [3][1120/1780], lr: 1.56250e-05, loss: 4.6733
Epoch [3][1125/1780], lr: 1.56250e-05, loss: 2.6268
Epoch [3][1130/1780], lr: 1.56250e-05, loss: 4.525
Epoch [3][1135/1780], lr: 1.56250e-05, loss: 5.0828
Epoch [3][1140/1780], lr: 1.56250e-05, loss: 4.3493
Epoch [3][1145/1780], lr: 1.56250e-05, loss: 3.4825
Epoch [3][1150/1780], lr: 1.56250e-05, loss: 3.7406
Epoch [3][1155/1780], lr: 1.56250e-05, loss: 3.7668
Epoch [3][1160/1780], lr: 1.56250e-05, loss: 2.7207
Epoch [3][1165/1780], lr: 1.56250e-05, loss: 5.0362
Epoch [3][1170/1780], lr: 1.56250e-05, loss: 3.5445
Epoch [3][1175/1780], lr: 1.56250e-05, loss: 4.2792
Epoch [3][1180/1780], lr: 1.56250e-05, loss: 3.7746
Epoch [3][1185/1780], lr: 1.56250e-05, loss: 1.9754
Epoch [3][1190/1780], lr: 1.56250e-05, loss: 3.1123
Epoch [3][1195/1780], lr: 1.56250e-05, loss: 4.4379
Epoch [3][1200/1780], lr: 1.56250e-05, loss: 3.4319
Epoch [3][1205/1780], lr: 1.56250e-05, loss: 3.4693
Epoch [3][1210/1780], lr: 1.56250e-05, loss: 2.7379
Epoch [3][1215/1780], lr: 1.56250e-05, loss: 3.7827
Epoch [3][1220/1780], lr: 1.56250e-05, loss: 4.8325
Epoch [3][1225/1780], lr: 1.56250e-05, loss: 3.1522
Epoch [3][1230/1780], lr: 1.56250e-05, loss: 2.8732
Epoch [3][1235/1780], lr: 1.56250e-05, loss: 3.8284
Epoch [3][1240/1780], lr: 1.56250e-05, loss: 4.7642
Epoch [3][1245/1780], lr: 1.56250e-05, loss: 4.2608
Epoch [3][1250/1780], lr: 1.56250e-05, loss: 3.4527
Epoch [3][1255/1780], lr: 1.56250e-05, loss: 3.5931
Epoch [3][1260/1780], lr: 1.56250e-05, loss: 4.3327
Epoch [3][1265/1780], lr: 1.56250e-05, loss: 3.7688
Epoch [3][1270/1780], lr: 1.56250e-05, loss: 3.1607
Epoch [3][1275/1780], lr: 1.56250e-05, loss: 3.953
Epoch [3][1280/1780], lr: 1.56250e-05, loss: 3.3821
Epoch [3][1285/1780], lr: 1.56250e-05, loss: 2.7326
Epoch [3][1290/1780], lr: 1.56250e-05, loss: 2.667
Epoch [3][1295/1780], lr: 1.56250e-05, loss: 3.4777
Epoch [3][1300/1780], lr: 1.56250e-05, loss: 2.9821
Epoch [3][1305/1780], lr: 1.56250e-05, loss: 3.5781
Epoch [3][1310/1780], lr: 1.56250e-05, loss: 3.1818
Epoch [3][1315/1780], lr: 1.56250e-05, loss: 3.9488
Epoch [3][1320/1780], lr: 1.56250e-05, loss: 3.6935
Epoch [3][1325/1780], lr: 1.56250e-05, loss: 3.4913
Epoch [3][1330/1780], lr: 1.56250e-05, loss: 3.5375
Epoch [3][1335/1780], lr: 1.56250e-05, loss: 4.1263
Epoch [3][1340/1780], lr: 1.56250e-05, loss: 2.7774
Epoch [3][1345/1780], lr: 1.56250e-05, loss: 2.6544
Epoch [3][1350/1780], lr: 1.56250e-05, loss: 4.3058
Epoch [3][1355/1780], lr: 1.56250e-05, loss: 3.2857
Epoch [3][1360/1780], lr: 1.56250e-05, loss: 1.6067
Epoch [3][1365/1780], lr: 1.56250e-05, loss: 3.6753
Epoch [3][1370/1780], lr: 1.56250e-05, loss: 3.536
Epoch [3][1375/1780], lr: 1.56250e-05, loss: 3.6943
Epoch [3][1380/1780], lr: 1.56250e-05, loss: 3.0741
Epoch [3][1385/1780], lr: 1.56250e-05, loss: 3.1761
Epoch [3][1390/1780], lr: 1.56250e-05, loss: 2.8075
Epoch [3][1395/1780], lr: 1.56250e-05, loss: 2.8499
Epoch [3][1400/1780], lr: 1.56250e-05, loss: 4.5139
Epoch [3][1405/1780], lr: 1.56250e-05, loss: 3.8172
Epoch [3][1410/1780], lr: 1.56250e-05, loss: 4.9965
Epoch [3][1415/1780], lr: 1.56250e-05, loss: 3.3139
Epoch [3][1420/1780], lr: 1.56250e-05, loss: 2.8437
Epoch [3][1425/1780], lr: 1.56250e-05, loss: 2.3215
Epoch [3][1430/1780], lr: 1.56250e-05, loss: 4.9746
Epoch [3][1435/1780], lr: 1.56250e-05, loss: 3.3591
Epoch [3][1440/1780], lr: 1.56250e-05, loss: 2.2411
Epoch [3][1445/1780], lr: 1.56250e-05, loss: 3.1519
Epoch [3][1450/1780], lr: 1.56250e-05, loss: 3.6207
Epoch [3][1455/1780], lr: 1.56250e-05, loss: 3.9792
Epoch [3][1460/1780], lr: 1.56250e-05, loss: 4.1067
Epoch [3][1465/1780], lr: 1.56250e-05, loss: 1.3949
Epoch [3][1470/1780], lr: 1.56250e-05, loss: 3.9147
Epoch [3][1475/1780], lr: 1.56250e-05, loss: 4.0652
Epoch [3][1480/1780], lr: 1.56250e-05, loss: 4.0677
Epoch [3][1485/1780], lr: 1.56250e-05, loss: 3.4798
Epoch [3][1490/1780], lr: 1.56250e-05, loss: 2.7119
Epoch [3][1495/1780], lr: 1.56250e-05, loss: 4.5059
Epoch [3][1500/1780], lr: 1.56250e-05, loss: 3.1044
Epoch [3][1505/1780], lr: 1.56250e-05, loss: 4.4929
Epoch [3][1510/1780], lr: 1.56250e-05, loss: 4.2862
Traceback (most recent call last):
  File "train_one_neuron_head.py", line 260, in <module>
    avg_loss, learning_rate = train_one_epoch(epoch+1)
  File "train_one_neuron_head.py", line 63, in train_one_epoch
    outputs = model(rgb=rgb,
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/Desktop/sign-language-summer-research/experiments/seven_sees/model/sees7.py", line 40, in forward
    stream = self.multistream_backbone(rgb=rgb,
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/Desktop/sign-language-summer-research/experiments/seven_sees/model/multistream_backbone.py", line 196, in forward
    stream['left_hand'] = self.left_hand_stream(rgb=left_hand)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/Desktop/sign-language-summer-research/experiments/seven_sees/model/flow_autoencoder.py", line 17, in forward
    rgb = self.rgb_backbone(rgb)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/Desktop/sign-language-summer-research/experiments/seven_sees/mmcv_model/mmcv_csn.py", line 870, in forward
    x = res_layer(x)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/Desktop/sign-language-summer-research/experiments/seven_sees/mmcv_model/mmcv_csn.py", line 332, in forward
    out = _inner_forward(x)
  File "/home/myuser1/Desktop/sign-language-summer-research/experiments/seven_sees/mmcv_model/mmcv_csn.py", line 320, in _inner_forward
    out = self.conv2(out)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/miniconda3/envs/mmsign/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py", line 209, in forward
    x = self.norm(x)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/myuser1/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
KeyboardInterrupt