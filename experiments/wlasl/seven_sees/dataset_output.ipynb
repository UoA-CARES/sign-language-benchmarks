{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a09a50b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1786c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myuser1/miniconda3/envs/wlasl/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset import MultiModalDataset\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from mmaction.datasets import build_dataset\n",
    "import torch\n",
    "import os\n",
    "import mmcv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ed7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde811b",
   "metadata": {},
   "source": [
    "## Function to display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c97a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, idx=0):\n",
    "    plt.imshow(tensor.permute(1, 2, 3, 0)[idx])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c17e0b",
   "metadata": {},
   "source": [
    "## Multimodal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66593840",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../data/wlasl/test_annotations.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultiModalDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../../../data/wlasl/test_annotations.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../../../data/wlasl/rawframes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mclip_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mframe_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43minput_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnum_clips\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     13\u001b[0m                                           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     14\u001b[0m                                           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m                                           num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     16\u001b[0m                                           pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/sign-language-summer-research/experiments/wlasl/seven_sees/analysis/dataset/dataset.py:73\u001b[0m, in \u001b[0;36mMultiModalDataset.__init__\u001b[0;34m(self, ann_file, root_dir, clip_len, resolution, input_resolution, transforms, frame_interval, num_clips, modalities, rgb_prefix, flow_prefix, depth_prefix, test_mode)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalise \u001b[38;5;241m=\u001b[39m Normalise(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_normalise \u001b[38;5;241m=\u001b[39m Normalise(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.9444415\u001b[39m, \u001b[38;5;241m0.9504853\u001b[39m, \u001b[38;5;241m0.9530699\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m0.1113386\u001b[39m, \u001b[38;5;241m0.1044944\u001b[39m, \u001b[38;5;241m0.1007349\u001b[39m])\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_pose \u001b[38;5;241m=\u001b[39m ReadPose()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_frames \u001b[38;5;241m=\u001b[39m SampleFrames(clip_len\u001b[38;5;241m=\u001b[39mclip_len,\n\u001b[1;32m     76\u001b[0m                                 frame_interval\u001b[38;5;241m=\u001b[39mframe_interval,\n\u001b[1;32m     77\u001b[0m                                 num_clips\u001b[38;5;241m=\u001b[39mnum_clips,\n\u001b[1;32m     78\u001b[0m                                 test_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_mode)\n",
      "File \u001b[0;32m~/Desktop/sign-language-summer-research/experiments/wlasl/seven_sees/analysis/dataset/dataset.py:103\u001b[0m, in \u001b[0;36mMultiModalDataset.load_annotations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"Load annotation file to get video information.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m video_infos \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mann_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fin):\n\u001b[1;32m    105\u001b[0m         line_split \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../data/wlasl/test_annotations.txt'"
     ]
    }
   ],
   "source": [
    "test_dataset = MultiModalDataset(ann_file='data/wlasl/test_annotations.txt',\n",
    "                                 root_dir='data/wlasl/rawframes',\n",
    "                                 clip_len=32,\n",
    "                                 resolution=224,\n",
    "                                 modalities=('rgb'),\n",
    "                                 test_mode=True,\n",
    "                                 frame_interval=1,\n",
    "                                 input_resolution=256,\n",
    "                                 num_clips=1)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4,\n",
    "                                          pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eca0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_dataset.load_video(1)\n",
    "results['frame_inds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a626ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_)\n",
    "i=0\n",
    "imshow(batch['flow'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080efa13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e09ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d83f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape, batch[1].shape, batch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc21a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(1,32,256,256).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259426a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3038bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat((x,x,x), dim=1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035446a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8569c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = batch[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4920254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fb4a2",
   "metadata": {},
   "source": [
    "## Finding the normalise values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# r_mean = 0\n",
    "# g_mean = 0\n",
    "# b_mean = 0\n",
    "\n",
    "# r_std = 0\n",
    "# g_std = 0\n",
    "# b_std = 0\n",
    "# iter_ = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf51554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while i < len(train_loader):\n",
    "#     batch = next(iter_)\n",
    "#     rgb = batch[0][0]\n",
    "\n",
    "#     r_mean += rgb[0].mean().item()\n",
    "#     g_mean += rgb[1].mean().item()\n",
    "#     b_mean += rgb[2].mean().item()\n",
    "\n",
    "#     r_std += rgb[0].std().item()\n",
    "#     g_std += rgb[1].std().item()\n",
    "#     b_std += rgb[2].std().item()\n",
    "\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_mean/i, g_mean/i, b_mean/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_std/i, g_std/i, b_std/i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a8ef8",
   "metadata": {},
   "source": [
    "## MMCV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c20148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset\n",
    "train_cfg = dict(\n",
    "    type='RawframeDataset',\n",
    "    ann_file='data/wlasl/train_annotations.txt',\n",
    "    data_prefix='data/wlasl/rawframes',\n",
    "    pipeline=[\n",
    "        dict(\n",
    "            type='SampleFrames',\n",
    "            clip_len=32,\n",
    "            frame_interval=2,\n",
    "            num_clips=1),\n",
    "        dict(type='RawFrameDecode'),\n",
    "        dict(type='Resize', scale=(-1, 256)),\n",
    "#         dict(type='RandomResizedCrop', area_range=(0.4, 1.0)),\n",
    "        dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
    "#         dict(type='Flip', flip_ratio=0.5),\n",
    "        dict(\n",
    "            type='Normalize',\n",
    "            mean=[123.675, 116.28, 103.53],\n",
    "            std=[58.395, 57.12, 57.375],\n",
    "            to_bgr=False),\n",
    "        dict(type='FormatShape', input_format='NCTHW'),\n",
    "        dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "        dict(type='ToTensor', keys=['imgs', 'label'])\n",
    "    ])\n",
    "\n",
    "train_dataset = build_dataset(train_cfg)\n",
    "\n",
    "test_cfg = dict(\n",
    "        type='RawframeDataset',\n",
    "        ann_file='data/wlasl/test_annotations.txt',\n",
    "        data_prefix='data/wlasl/rawframes',\n",
    "        test_mode=True,\n",
    "        pipeline=[\n",
    "            dict(\n",
    "                    type='SampleFrames',\n",
    "                    clip_len=32,\n",
    "                    frame_interval=2,\n",
    "                    num_clips=1,\n",
    "                    test_mode=True),\n",
    "            dict(type='RawFrameDecode'),\n",
    "            dict(type='Resize', scale=(-1, 256)),\n",
    "            dict(type='CenterCrop', crop_size=224),\n",
    "            dict(\n",
    "                type='Normalize',\n",
    "                mean=[123.675, 116.28, 103.53],\n",
    "                std=[58.395, 57.12, 57.375],\n",
    "                to_bgr=False),\n",
    "            dict(type='FormatShape', input_format='NCTHW'),\n",
    "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "            dict(type='ToTensor', keys=['imgs'])\n",
    "        ])\n",
    "\n",
    "# Building the datasets\n",
    "test_dataset = build_dataset(test_cfg)\n",
    "\n",
    "mmcv_test_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=4,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_iter = iter(mmcv_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(img_iter)\n",
    "mmcv_imgs = x['imgs']\n",
    "imshow(mmcv_imgs.squeeze(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_)\n",
    "i=0\n",
    "imshow(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ad216",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_iter = iter(mmcv_imgs.squeeze().permute(1,2,3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e038b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(video_iter))\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wlasl)",
   "language": "python",
   "name": "wlasl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
