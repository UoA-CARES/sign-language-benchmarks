{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a09a50b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1786c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset import MultiModalDataset\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from mmaction.datasets import build_dataset\n",
    "import torch\n",
    "import os\n",
    "import mmcv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ed7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde811b",
   "metadata": {},
   "source": [
    "## Function to display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c97a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, idx=0):\n",
    "    plt.imshow(tensor.permute(1, 2, 3, 0)[idx])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c17e0b",
   "metadata": {},
   "source": [
    "## Multimodal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66593840",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiModalDataset(ann_file='data/wlasl/train_annotations.txt',\n",
    "                                  root_dir='data/wlasl/rawframes',\n",
    "                                  clip_len=32,\n",
    "                                  resolution=224,\n",
    "                                  modalities=('rgb', 'flow'),\n",
    "                                  frame_interval=1,\n",
    "                                  num_clips=1,\n",
    "                                  test_mode=True\n",
    "                                  )\n",
    "\n",
    "test_dataset = MultiModalDataset(ann_file='data/wlasl/test_annotations.txt',\n",
    "                                 root_dir='data/wlasl/rawframes',\n",
    "                                 clip_len=32,\n",
    "                                 resolution=224,\n",
    "                                 modalities=('rgb', 'flow'),\n",
    "                                 test_mode=True,\n",
    "                                 frame_interval=1,\n",
    "                                 num_clips=1\n",
    "                                 )\n",
    "\n",
    "# Setting up dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=4,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4,\n",
    "                                          pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69eca0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
       "       26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = train_dataset.load_video(1)\n",
    "results['frame_inds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a626ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7db3640",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 32 is out of bounds for dimension 0 with size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_)\n\u001b[1;32m      2\u001b[0m i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(tensor, idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(tensor, idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 32 is out of bounds for dimension 0 with size 32"
     ]
    }
   ],
   "source": [
    "batch = next(iter_)\n",
    "i=0\n",
    "imshow(batch[0][0], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b75edfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8569c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = batch[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4920254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fb4a2",
   "metadata": {},
   "source": [
    "## Finding the normalise values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# r_mean = 0\n",
    "# g_mean = 0\n",
    "# b_mean = 0\n",
    "\n",
    "# r_std = 0\n",
    "# g_std = 0\n",
    "# b_std = 0\n",
    "# iter_ = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf51554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while i < len(train_loader):\n",
    "#     batch = next(iter_)\n",
    "#     rgb = batch[0][0]\n",
    "\n",
    "#     r_mean += rgb[0].mean().item()\n",
    "#     g_mean += rgb[1].mean().item()\n",
    "#     b_mean += rgb[2].mean().item()\n",
    "\n",
    "#     r_std += rgb[0].std().item()\n",
    "#     g_std += rgb[1].std().item()\n",
    "#     b_std += rgb[2].std().item()\n",
    "\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_mean/i, g_mean/i, b_mean/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_std/i, g_std/i, b_std/i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a8ef8",
   "metadata": {},
   "source": [
    "## MMCV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c20148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset\n",
    "train_cfg = dict(\n",
    "    type='RawframeDataset',\n",
    "    ann_file='data/wlasl/train_annotations.txt',\n",
    "    data_prefix='data/wlasl/rawframes',\n",
    "    pipeline=[\n",
    "        dict(\n",
    "            type='SampleFrames',\n",
    "            clip_len=32,\n",
    "            frame_interval=2,\n",
    "            num_clips=1),\n",
    "        dict(type='RawFrameDecode'),\n",
    "        dict(type='Resize', scale=(-1, 256)),\n",
    "#         dict(type='RandomResizedCrop', area_range=(0.4, 1.0)),\n",
    "        dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
    "#         dict(type='Flip', flip_ratio=0.5),\n",
    "        dict(\n",
    "            type='Normalize',\n",
    "            mean=[123.675, 116.28, 103.53],\n",
    "            std=[58.395, 57.12, 57.375],\n",
    "            to_bgr=False),\n",
    "        dict(type='FormatShape', input_format='NCTHW'),\n",
    "        dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "        dict(type='ToTensor', keys=['imgs', 'label'])\n",
    "    ])\n",
    "\n",
    "train_dataset = build_dataset(train_cfg)\n",
    "\n",
    "test_cfg = dict(\n",
    "        type='RawframeDataset',\n",
    "        ann_file='data/wlasl/test_annotations.txt',\n",
    "        data_prefix='data/wlasl/rawframes',\n",
    "        test_mode=True,\n",
    "        pipeline=[\n",
    "            dict(\n",
    "                    type='SampleFrames',\n",
    "                    clip_len=32,\n",
    "                    frame_interval=2,\n",
    "                    num_clips=1,\n",
    "                    test_mode=True),\n",
    "            dict(type='RawFrameDecode'),\n",
    "            dict(type='Resize', scale=(-1, 256)),\n",
    "            dict(type='CenterCrop', crop_size=224),\n",
    "            dict(\n",
    "                type='Normalize',\n",
    "                mean=[123.675, 116.28, 103.53],\n",
    "                std=[58.395, 57.12, 57.375],\n",
    "                to_bgr=False),\n",
    "            dict(type='FormatShape', input_format='NCTHW'),\n",
    "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "            dict(type='ToTensor', keys=['imgs'])\n",
    "        ])\n",
    "\n",
    "# Building the datasets\n",
    "test_dataset = build_dataset(test_cfg)\n",
    "\n",
    "mmcv_test_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=4,\n",
    "                                      pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_iter = iter(mmcv_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(img_iter)\n",
    "mmcv_imgs = x['imgs']\n",
    "imshow(mmcv_imgs.squeeze(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_)\n",
    "i=0\n",
    "imshow(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ad216",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_iter = iter(mmcv_imgs.squeeze().permute(1,2,3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e038b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(video_iter))\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataloader",
   "language": "python",
   "name": "dataloader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
